{
 "cells": [
  {
   "cell_type": "code",
   "id": "bfeb1b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:03:00.950169Z",
     "start_time": "2025-10-06T20:03:00.944575Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "b2644b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:03:01.486664Z",
     "start_time": "2025-10-06T20:03:01.483139Z"
    }
   },
   "source": "# Input file after dropping rows with missing Total Spent\nCSV_IN = \"../../output/1_handle_missing_data/price_per_unit_reconstructed.csv\"\nCSV_OUT = \"../../output/1_handle_missing_data/item_imputed.csv\"\n\n# Define columns name\nTOTAL_SPENT = \"Total Spent\"\nPRICE_PER_UNIT = \"Price Per Unit\"\nQUANTITY = \"Quantity\"\nCATEGORY = \"Category\"\nPAYMENT_METHOD = \"Payment Method\"\nLOCATION = \"Location\"\nITEM = \"Item\"\nTRANSACTION_ID = \"Transaction ID\"\n\n# Define error\nCOERCE_ERRORS = \"coerce\"",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "965cec7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:03:02.223343Z",
     "start_time": "2025-10-06T20:03:02.199119Z"
    }
   },
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(CSV_IN)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "1d4e2fd2",
   "metadata": {},
   "source": [
    "### Quantify missing Item\n",
    "\n",
    "Determine the scale of missing entries in `Item` after STEP 2 completion. Since Item cannot be deterministically reconstructed (no formula exists), statistical imputation using Category information will be required.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count missing Item values\n",
    "missingItem = df[ITEM].isna()\n",
    "print(f'Missing Item rows: {missingItem.sum()} of {len(df)} ({missingItem.mean():.2%})')\n",
    "\n",
    "# Verify that numeric columns are complete\n",
    "print(f'\\nVerification of previous steps:')\n",
    "print(f'  Total Spent missing: {df[TOTAL_SPENT].isna().sum()}')\n",
    "print(f'  Quantity missing: {df[QUANTITY].isna().sum()}')\n",
    "print(f'  Price Per Unit missing: {df[PRICE_PER_UNIT].isna().sum()}')\n"
   ],
   "id": "34c5a159fda6ac64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missingness mechanism\n",
    "\n",
    "Quantifying how often `Item` is missing within each `Category`, payment method, and location to understand the systematic pattern. The variation across categories confirms MAR classification.\n"
   ],
   "id": "268df20c81979547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze missingness patterns across categories\n",
    "summary = df.assign(missingItem=missingItem).groupby(CATEGORY)['missingItem'].mean().sort_values(ascending=False) * 100\n",
    "print('Share of Item missing by Category:')\n",
    "print(summary.round(2).astype(str) + '%')\n",
    "\n",
    "# Analyze missingness patterns across payment methods\n",
    "paymentShare = df.assign(missingItem=missingItem).groupby(PAYMENT_METHOD)['missingItem'].mean().sort_values(ascending=False) * 100\n",
    "print('\\nShare of Item missing by Payment Method:')\n",
    "print(paymentShare.round(2).astype(str) + '%')\n",
    "\n",
    "# Analyze missingness patterns across locations\n",
    "locationShare = df.assign(missingItem=missingItem).groupby(LOCATION)['missingItem'].mean().sort_values(ascending=False) * 100\n",
    "print('\\nShare of Item missing by Location:')\n",
    "print(locationShare.round(2).astype(str) + '%')\n"
   ],
   "id": "8eaeb1b1feeae0a0"
  },
  {
   "cell_type": "markdown",
   "id": "0f26c518",
   "metadata": {},
   "source": [
    "### Category coverage analysis\n",
    "\n",
    "Verify that all missing Item rows have valid Category information, which is essential for category-based imputation.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if all missing Item rows have valid Category\n",
    "itemAndCategoryMissing = (missingItem & df[CATEGORY].isna()).sum()\n",
    "\n",
    "print(f'Rows with both Item and Category missing: {itemAndCategoryMissing}')\n",
    "print(f'Rows with Item missing but Category present: {missingItem.sum() - itemAndCategoryMissing}')\n",
    "print(f'Category coverage for missing Items: {((missingItem.sum() - itemAndCategoryMissing) / missingItem.sum() * 100):.1f}%')\n"
   ],
   "id": "874eeaaf4cb2a2f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Item distribution analysis\n",
    "\n",
    "Examine the distribution of Items within each Category to understand what values are available for imputation. The mode (most frequent item) per category will be used.\n"
   ],
   "id": "694e8906f953b918"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:04:55.982736Z",
     "start_time": "2025-10-06T20:04:55.965775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze Item distribution within each Category\n",
    "# Calculate mode (most frequent item) for each category\n",
    "# We'll use this for imputation\n",
    "for category in df[CATEGORY].unique():\n",
    "    # Filter data to current category and non-missing Items\n",
    "    categoryData = df[(df[CATEGORY] == category) & (df[ITEM].notna())]\n",
    "    \n",
    "    if len(categoryData) > 0:\n",
    "        # .mode() returns the most frequent value(s)\n",
    "        # [0] takes the first mode if there are multiple\n",
    "        modeItem = categoryData[ITEM].mode()[0]\n",
    "        # Count how many times this item appears\n",
    "        modeCount = (categoryData[ITEM] == modeItem).sum()\n",
    "        # Calculate percentage\n",
    "        modePct = (modeCount / len(categoryData)) * 100\n",
    "        # Count how many Items will be imputed for this category\n",
    "        toImpute = ((df[CATEGORY] == category) & missingItem).sum()\n",
    "        \n",
    "        print(f'{category:40s}: {modeItem:20s} (appears {modeCount:4d} times, {modePct:5.1f}%) -> will impute {toImpute} rows')\n",
    "\n",
    "# Show unique item counts per category\n",
    "print('\\nItem variety per Category:')\n",
    "itemVariety = df[df[ITEM].notna()].groupby(CATEGORY)[ITEM].nunique().sort_values(ascending=False)\n",
    "for category, count in itemVariety.items():\n",
    "    print(f'{category:40s}: {count:3d} unique items')\n"
   ],
   "id": "fdb9ce5ddd8ad594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food                                    : Item_14_FOOD         (appears  106 times,   7.4%) -> will impute 81 rows\n",
      "Furniture                               : Item_25_FUR          (appears  113 times,   7.7%) -> will impute 65 rows\n",
      "Computers and electric accessories      : Item_19_CEA          (appears  106 times,   7.6%) -> will impute 80 rows\n",
      "Milk Products                           : Item_16_MILK         (appears  109 times,   7.6%) -> will impute 88 rows\n",
      "Electric household essentials           : Item_8_EHE           (appears  105 times,   7.3%) -> will impute 79 rows\n",
      "Beverages                               : Item_2_BEV           (appears  126 times,   8.8%) -> will impute 69 rows\n",
      "Butchers                                : Item_20_BUT          (appears  107 times,   7.5%) -> will impute 75 rows\n",
      "Patisserie                              : Item_12_PAT          (appears  100 times,   7.3%) -> will impute 72 rows\n",
      "\n",
      "Item variety per Category:\n",
      "Beverages                               :  25 unique items\n",
      "Butchers                                :  25 unique items\n",
      "Computers and electric accessories      :  25 unique items\n",
      "Electric household essentials           :  25 unique items\n",
      "Food                                    :  25 unique items\n",
      "Furniture                               :  25 unique items\n",
      "Milk Products                           :  25 unique items\n",
      "Patisserie                              :  25 unique items\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing data classification\n",
    "\n",
    "**Classification: MAR (Missing At Random)**\n",
    "\n",
    "**Rationale:**\n",
    "- ALL 609 missing Items have valid Category information (100% coverage)\n",
    "- The missingness depends on Category (an observable variable)\n",
    "- Not MCAR because missing rates are not uniform across categories\n",
    "- Not MNAR because missingness is explained by Category, not by the item values themselves\n",
    "\n"
   ],
   "id": "f63cdb2d859ad7a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling strategy: Mode imputation by Category\n",
    "\n",
    "**Reason for choose mode imputation (not mean/median or deletion):**\n",
    "\n",
    "1. **Strong predictor available:** Category provides strong signal for Item prediction (100% coverage)\n",
    "2. **Preserves distribution:** Mode imputation maintains the frequency distribution within each category\n",
    "3. **Conservative approach:** Uses actual existing item codes, doesn't create synthetic values\n",
    "4. **Appropriate for categorical df:** Mode is the standard measure for categorical variables\n",
    "5. **Small data loss if deleted:** Dropping 609 rows (5.09%) would lose valuable data unnecessarily\n",
    "\n",
    "**Why mode (not other methods):**\n",
    "- **Mean/Median:** Not applicable to categorical (text) data\n",
    "- **Deletion:** Would lose 5.09% of data when imputation is feasible\n",
    "- **Random imputation:** Less stable than mode, introduces unnecessary variance\n",
    "- **Create new category:** Would violate existing item naming convention\n",
    "- **Predictive model:** More complex, may overfit with limited missing df\n",
    "\n",
    "\n"
   ],
   "id": "abdfe1ca2f43ac53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:07:02.532115Z",
     "start_time": "2025-10-06T20:07:02.520112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(df[missingItem][[TRANSACTION_ID, CATEGORY, ITEM, PRICE_PER_UNIT, QUANTITY, TOTAL_SPENT]].head(10))\n"
   ],
   "id": "3d5dd7da5997cdd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Transaction ID                            Category Item  Price Per Unit  \\\n",
      "12     TXN_1007496                            Butchers  NaN            15.5   \n",
      "50     TXN_1032287                                Food  NaN            21.5   \n",
      "68     TXN_1044590       Electric household essentials  NaN            14.0   \n",
      "70     TXN_1046262                       Milk Products  NaN            14.0   \n",
      "71     TXN_1046367  Computers and electric accessories  NaN            18.5   \n",
      "76     TXN_1051223                          Patisserie  NaN             5.0   \n",
      "87     TXN_1058643                                Food  NaN             9.5   \n",
      "104    TXN_1071762                           Beverages  NaN             9.5   \n",
      "134    TXN_1095879                           Beverages  NaN             6.5   \n",
      "136    TXN_1096977                                Food  NaN            23.0   \n",
      "\n",
      "     Quantity  Total Spent  \n",
      "12       10.0        155.0  \n",
      "50        2.0         43.0  \n",
      "68        4.0         56.0  \n",
      "70        5.0         70.0  \n",
      "71       10.0        185.0  \n",
      "76        9.0         45.0  \n",
      "87        2.0         19.0  \n",
      "104       3.0         28.5  \n",
      "134      10.0         65.0  \n",
      "136       9.0        207.0  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:07:37.595439Z",
     "start_time": "2025-10-06T20:07:37.573589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count missing values before imputation\n",
    "itemMissingBefore = missingItem.sum()\n",
    "print(f'Item missing before imputation: {itemMissingBefore}')\n",
    "\n",
    "# Perform mode imputation by Category\n",
    "# Create a dictionary to track imputation details for reporting\n",
    "imputationDetails = []\n",
    "\n",
    "# Iterate through each unique category\n",
    "# df[CATEGORY].unique() returns all unique category values\n",
    "for category in df[CATEGORY].unique():\n",
    "    # Create filter for rows in this category with missing Item\n",
    "    # (df[CATEGORY] == category) filters to current category\n",
    "    # & missing_item filters to rows with missing Item\n",
    "    # Both conditions must be True\n",
    "    categoryMask = (df[CATEGORY] == category) & missingItem\n",
    "    missingInCategory = categoryMask.sum()\n",
    "    \n",
    "    if missingInCategory > 0:\n",
    "        # Get non-missing Items in this category to find the mode\n",
    "        # Filter to current category AND non-missing Items\n",
    "        categoryItems = df[(df[CATEGORY] == category) & (df[ITEM].notna())][ITEM]\n",
    "        \n",
    "        if len(categoryItems) > 0:\n",
    "            # .mode() returns the most frequent value(s) as a Series\n",
    "            # [0] takes the first mode if there are multiple modes\n",
    "            modeItem = categoryItems.mode()[0]\n",
    "            \n",
    "            # Perform imputation: assign mode_item to all missing Items in this category\n",
    "            # .loc[filter, column] allows us to update specific rows and columns\n",
    "            df.loc[categoryMask, ITEM] = modeItem\n",
    "            \n",
    "            # Track details for reporting\n",
    "            imputationDetails.append({\n",
    "                'Category': category,\n",
    "                'Missing Count': missingInCategory,\n",
    "                'Imputed With': modeItem\n",
    "            })\n",
    "\n",
    "# Count missing values after imputation\n",
    "# df[ITEM].isna().sum() recounts missing values after imputation\n",
    "itemMissingAfter = df[ITEM].isna().sum()\n",
    "# Calculate how many values were successfully imputed\n",
    "valuesImputed = itemMissingBefore - itemMissingAfter\n",
    "\n",
    "print(f'\\nItem missing after imputation: {itemMissingAfter}')\n",
    "print(f'Values successfully imputed: {valuesImputed}')\n",
    "\n",
    "\n",
    "# Display imputation details\n",
    "print('Imputation Details by Category:')\n",
    "\n",
    "# Create DataFrame from imputation details for formatting\n",
    "imputation_df = pd.DataFrame(imputationDetails)\n",
    "print(imputation_df.to_string(index=False))\n"
   ],
   "id": "4e60e455a7f509a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item missing before imputation: 609\n",
      "\n",
      "Item missing after imputation: 0\n",
      "Values successfully imputed: 609\n",
      "Imputation Details by Category:\n",
      "                          Category  Missing Count Imputed With\n",
      "                              Food             81 Item_14_FOOD\n",
      "                         Furniture             65  Item_25_FUR\n",
      "Computers and electric accessories             80  Item_19_CEA\n",
      "                     Milk Products             88 Item_16_MILK\n",
      "     Electric household essentials             79   Item_8_EHE\n",
      "                         Beverages             69   Item_2_BEV\n",
      "                          Butchers             75  Item_20_BUT\n",
      "                        Patisserie             72  Item_12_PAT\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "dff73dcf",
   "metadata": {},
   "source": [
    "### Validation: Verify imputation correctness\n",
    "\n",
    "Verify that the imputed Item values maintain category consistency and that all items follow the correct naming convention\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:08:38.376391Z",
     "start_time": "2025-10-06T20:08:38.373519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify Item is now complete\n",
    "print('Missing value check after imputation:')\n",
    "print(f'Item missing: {df[ITEM].isna().sum()}')\n",
    "print(f'Price Per Unit missing: {df[PRICE_PER_UNIT].isna().sum()}')\n",
    "print(f'Quantity missing: {df[QUANTITY].isna().sum()}')\n",
    "print(f'Total Spent missing: {df[TOTAL_SPENT].isna().sum()}')\n"
   ],
   "id": "5f477d6355b0b370",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value check after imputation:\n",
      "Item missing: 0\n",
      "Price Per Unit missing: 0\n",
      "Quantity missing: 0\n",
      "Total Spent missing: 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Impact on remaining missing values\n",
    "\n",
    "Analyze the current state of missing data after Item imputation. Only Discount Applied should have missing values remaining\n"
   ],
   "id": "66f0712a5809d68e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:09:34.917726Z",
     "start_time": "2025-10-06T20:09:34.910238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Current missing value status across all columns:')\n",
    "\n",
    "# Check all columns for missing values\n",
    "missingSummary = df.isnull().sum()\n",
    "# Filter to show only columns with missing values\n",
    "missingCols = missingSummary[missingSummary > 0]\n",
    "\n",
    "print(missingCols)\n"
   ],
   "id": "b1d3015a3a3f75b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current missing value status across all columns:\n",
      "Discount Applied    3988\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Persist results\n",
    "\n",
    "Save the dataset with Item imputed. This becomes the input for STEP 4 (Discount Applied handling)\n"
   ],
   "id": "3fa5403bfaea493b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save dataset with Item imputed to CSV\n",
    "df.to_csv(CSV_OUT, index=False)"
   ],
   "id": "a14abfe72e81f9fc"
  },
  {
   "cell_type": "markdown",
   "id": "86e418f3",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Item Handling**\n",
    "\n",
    "**Classification:** MAR (Missing At Random)\n",
    "- Missingness depends on Category (observable variable)\n",
    "- Missing rates vary by category (8.23%-10.41%)\n",
    "- ALL missing Items have valid Category information (100%)\n",
    "\n",
    "**Method:** Mode imputation by Category\n",
    "- Imputed 609 values (100% of missing items)\n",
    "- Used most frequent item within each category\n",
    "- Conservative approach using existing item codes\n",
    "\n",
    "**Justification:**\n",
    "- Category provides strong predictive signal (100% coverage)\n",
    "- Mode preserves distribution within categories\n",
    "- Appropriate method for categorical data\n",
    "- Maintains category consistency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94f56b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
