{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f353abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T17:39:20.934609Z",
     "start_time": "2025-10-04T17:39:20.728843Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4b846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T17:39:15.786970Z",
     "start_time": "2025-10-04T17:39:15.785376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input dataset\n",
    "CSV_IN = \"../../../datasource/Deliverable1Dataset.csv\"\n",
    "CSV_OUT = \"../output_data/1_total_spent/total_spent_cleaned.csv\"\n",
    "\n",
    "# Define columns name\n",
    "TOTAL_SPENT = \"Total Spent\"\n",
    "PRICE_PER_UNIT = \"Price Per Unit\"\n",
    "QUANTITY = \"Quantity\"\n",
    "CATEGORY = \"Category\"\n",
    "PAYMENT_METHOD = \"Payment Method\"\n",
    "LOCATION = \"Location\"\n",
    "ITEM = \"Item\"\n",
    "TRANSACTION_ID = \"Transaction ID\"\n",
    "\n",
    "# Define error\n",
    "COERCE_ERRORS = \"coerce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01609558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T17:39:16.840913Z",
     "start_time": "2025-10-04T17:39:16.828621Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Deliverable1Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_IN\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Deliverable1Dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2228ca8",
   "metadata": {},
   "source": [
    "### Determine missingness of Total Spent\n",
    "\n",
    "Before checking, convert all the existing data types (string) to appropriate numeric type for comparison the relationship since `Price Per Unit`, `Quantity` and `Total Spent` are related. Check to see how many missing observations for `Total Spent`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f887991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 12575\n",
      "Missing Total Spent: 604 (4.8%)\n"
     ]
    }
   ],
   "source": [
    "# Convert column's values to numeric, coercing errors to NaN\n",
    "# col is each of the relevant columns\n",
    "for col in [PRICE_PER_UNIT, QUANTITY, TOTAL_SPENT]:\n",
    "    # pd.to_numeric converts values in col to numeric, with errors coerced to NaN\n",
    "    # df[col] accesses the target column\n",
    "    # errors=COERCE_ERRORS specifies that parsing errors are set to NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors=COERCE_ERRORS)\n",
    "\n",
    "# Basic counts\n",
    "# rows is the total number of rows in df\n",
    "totalRow = len(df)\n",
    "# missingValue is the number of rows where TOTAL_SPENT is NaN\n",
    "# df[TOTAL_SPENT] accesses the target column\n",
    "# .isna() creates a boolean Series where True is NaN\n",
    "# .sum() counts the number of True values\n",
    "missingValue = df[TOTAL_SPENT].isna().sum()\n",
    "# missingPercent is the percentage of rows where TOTAL_SPENT is NaN\n",
    "# missingValue / totalRow computes the fraction of missing values\n",
    "missingPercent = round(missingValue / totalRow * 100, 2)\n",
    "\n",
    "print(f\"Total rows: {totalRow}\")\n",
    "print(f\"Missing {TOTAL_SPENT}: {missingValue} ({missingPercent}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6ec84",
   "metadata": {},
   "source": [
    "### Missingness mechanism\n",
    "\n",
    "Quantifying how often `Total Spent` is missing within each `Category`, `payment method`, and `location` to see their relationships with Total Spent if they were depended on or related to each other. From that can conclude what kind of missing data type like Missing Completely At Random (MCAR), Missing At Random (MAR), or Missing Not At Random (MNAR) and propose the appropriate approach to handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6807807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Total Spent by Category (%):\n",
      "Category\n",
      "Patisserie                            5.69\n",
      "Computers and electric accessories    5.20\n",
      "Food                                  5.10\n",
      "Electric household essentials         4.71\n",
      "Butchers                              4.59\n",
      "Beverages                             4.53\n",
      "Milk Products                         4.48\n",
      "Furniture                             4.15\n",
      "Name: Total Spent, dtype: float64\n",
      "\n",
      "Missing Total Spent by Payment Method (%):\n",
      "Payment Method\n",
      "Digital Wallet    4.90\n",
      "Cash              4.80\n",
      "Credit Card       4.71\n",
      "Name: Total Spent, dtype: float64\n",
      "\n",
      "Missing Total Spent by Location (%):\n",
      "Location\n",
      "In-store    5.11\n",
      "Online      4.50\n",
      "Name: Total Spent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missingness by Category\n",
    "# .groupby(CATEGORY)[TOTAL_SPENT] groups Total Spent by Category\n",
    "# .apply applies the lambda function to Category type group of Total Spent\n",
    "# lambda s: is a lambda function where x is each group of Category type group of Total Spent\n",
    "# s.isna() creates a boolean Series where NaN = True\n",
    "# .mean() computes the mean of the boolean Series\n",
    "# .sort_values(ascending=False) sorts the results in descending order\n",
    "# now byCategory stores the percentage of missing TOTAL_SPENT by CATEGORY\n",
    "# multiplying by 100 converts the fraction to percentage\n",
    "byCategory = df.groupby(CATEGORY)[TOTAL_SPENT].apply(lambda x: x.isna().mean()).sort_values(ascending=False) * 100\n",
    "print(\"\\nMissing Total Spent by Category (%):\")\n",
    "print(byCategory.round(2))\n",
    "\n",
    "# Missingness by Payment Method\n",
    "# .groupby(PAYMENT_METHOD)[TOTAL_SPENT] groups Total Spent by Payment Method\n",
    "# .apply applies the lambda function to each Payment Method type group of Total Spent\n",
    "# lambda s: is a lambda function where x is each group of Payment Method type of Total Spent\n",
    "# s.isna() creates a boolean Series where NaN = True\n",
    "# .mean() computes the mean of the boolean Series\n",
    "# .sort_values(ascending=False) sorts the results in descending order\n",
    "# now byPayment stores the percentage of missing TOTAL_SPENT by PAYMENT_METHOD\n",
    "# multiplying by 100 converts the fraction to percentage\n",
    "byPayment = df.groupby(PAYMENT_METHOD)[TOTAL_SPENT].apply(lambda x: x.isna().mean()).sort_values(ascending=False) * 100\n",
    "print(\"\\nMissing Total Spent by Payment Method (%):\")\n",
    "print(byPayment.round(2))\n",
    "\n",
    "# Missingness by Location\n",
    "# .groupby(LOCATION)[TOTAL_SPENT] groups Total Spent by Location\n",
    "# .apply applies the lambda function to each Location type group of Total Spent\n",
    "# lambda s: is a lambda function where x is each group of Location type of Total Spent\n",
    "# s.isna() creates a boolean Series where NaN = True\n",
    "# .mean() computes the mean of the boolean Series\n",
    "# .sort_values(ascending=False) sorts the results in descending order\n",
    "# now byLocation stores the percentage of missing TOTAL_SPENT by LOCATION\n",
    "# multiplying by 100 converts the fraction to percentage\n",
    "byLocation = df.groupby(LOCATION)[TOTAL_SPENT].apply(lambda x: x.isna().mean()).sort_values(ascending=False) * 100\n",
    "print(\"\\nMissing Total Spent by Location (%):\")\n",
    "print(byLocation.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47aae4",
   "metadata": {},
   "source": [
    "### Co-missingness analysis\n",
    "\n",
    "Examining whether `Total Spent` is missing alongside other key fields which determine the appropriate patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with both Total Spent and Quantity missing: 604\n",
      "Total Spent missing: 604\n",
      "Perfect overlap: True\n",
      "\n",
      "Rows with both Total Spent and Price Per Unit missing: 0\n",
      "Rows with both Total Spent and Item missing: 604\n",
      "Percentage of Total Spent missing cases with Item also missing: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# related missingness with Quantity\n",
    "# (df[TOTAL_SPENT].isna()) & (df[QUANTITY].isna()) creates a boolean Series where both are NaN\n",
    "relateMissingQty = ((df[TOTAL_SPENT].isna()) & (df[QUANTITY].isna())).sum()\n",
    "\n",
    "print(f'Rows with both Total Spent and Quantity missing: {relateMissingQty}')\n",
    "print(f'Total Spent missing: {missingValue}')\n",
    "print(f'Perfect overlap: {relateMissingQty == missingValue}')\n",
    "\n",
    "# Check co-missingness with Price Per Unit\n",
    "# (df[TOTAL_SPENT].isna()) & (df[PRICE_PER_UNIT].isna()) creates a boolean Series where both are NaN\n",
    "relateMissingPrice = ((df[TOTAL_SPENT].isna()) & (df[PRICE_PER_UNIT].isna())).sum()\n",
    "print(f'\\nRows with both Total Spent and Price Per Unit missing: {relateMissingPrice}')\n",
    "\n",
    "# Check co-missingness with Item\n",
    "# Count rows where both Total Spent and Item are missing\n",
    "relatedMissingItem = ((df[TOTAL_SPENT].isna()) & (df[ITEM].isna())).sum()\n",
    "print(f'Rows with both Total Spent and Item missing: {relatedMissingItem}')\n",
    "print(f'Percentage of Total Spent missing cases with Item also missing: {relatedMissingItem / missingValue * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e48072",
   "metadata": {},
   "source": [
    "### Reconstructability assessment\n",
    "\n",
    "Since `Total Spent = Price Per Unit × Quantity`, we assess how many missing `Total Spent` values could theoretically be reconstructed from the other two fields. This determines whether imputation is feasible or deletion is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Total Spent that CAN be reconstructed: 0 out of 604\n",
      "Reconstruction rate: 0.0%\n",
      "\n",
      "Missing Total Spent that CANNOT be reconstructed: 604 out of 604\n",
      "Irrecoverable rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check if Total Spent can be reconstructed from Price Per Unit and Quantity\n",
    "# For reconstruction, we need Total Spent to be missing BUT both Price and Quantity to be present\n",
    "# df[TOTAL_SPENT].isna ensures Total Spent is missing\n",
    "# df[PRICE_PER_UNIT].notna() ensures Price Per Unit is NOT missing\n",
    "# df[QUANTITY].notna() ensures Quantity is NOT missing\n",
    "# & combines all three conditions (all must be True)\n",
    "reconstructable = df[TOTAL_SPENT].isna() & df[PRICE_PER_UNIT].notna() & df[QUANTITY].notna()\n",
    "reconstructableCount = reconstructable.sum()\n",
    "\n",
    "print(f'Missing Total Spent that CAN be reconstructed: {reconstructableCount} out of {missingValue}')\n",
    "print(f'Reconstruction rate: {reconstructableCount / missingValue:.1%}')\n",
    "\n",
    "# Check irrecoverable cases (missing Total Spent AND at least one other field)\n",
    "# ~ negates reconstructable, giving us rows where reconstruction is NOT possible\n",
    "# These are rows where Total Spent is missing AND at least one of (Price, Quantity) is also missing\n",
    "irrecoverable = df[TOTAL_SPENT].isna() & ~reconstructable\n",
    "irrecoverableCount = irrecoverable.sum()\n",
    "\n",
    "print(f'\\nMissing Total Spent that CANNOT be reconstructed: {irrecoverableCount} out of {missingValue}')\n",
    "print(f'Irrecoverable rate: {irrecoverableCount / missingValue:.1%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b65276",
   "metadata": {},
   "source": [
    "### Missing data classification\n",
    "\n",
    "**Classification: MAR (Missing At Random)**\n",
    "\n",
    "**Rationale:**\n",
    "- Perfect co-missingness with `Quantity` (604 cases = 100% overlap)\n",
    "- Missing rates vary by category (4.15%-5.69%), indicating dependence on observable characteristics\n",
    "- Strongly correlated with `Item` field missingness (100% of cases have Item also missing)\n",
    "- The missingness is systematic and related to the Item field (an observable variable)\n",
    "- Not MCAR because missing rates are not uniform across categories\n",
    "- Not MNAR because the missingness is explained by observable variables (Item field status)\n",
    "\n",
    "**Key finding:** When `Item` was not recorded during data collection, both `Quantity` and `Total Spent` were also systematically omitted, suggesting a data entry workflow issue rather than values being hidden due to their magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25116ba7",
   "metadata": {},
   "source": [
    "### Handling strategy: Listwise deletion\n",
    "\n",
    "**Justification for deletion (not imputation):**\n",
    "\n",
    "1. **Critical target variable:** `Total Spent` is essential for transaction analysis and should not be estimated\n",
    "2. **Perfect co-missingness:** These 604 rows also have missing `Quantity`, making reconstruction impossible\n",
    "3. **Cannot reliably impute:** Missing both Quantity AND at least one other key field\n",
    "4. **Small data loss:** Only 4.8% of the dataset vs. large gain in data integrity\n",
    "5. **Side benefit:** Eliminates 604 problematic cases that would require multiple imputations\n",
    "\n",
    "**Alternative considered:** Reconstruct using Price × Quantity\n",
    "- Not feasible: 0% of missing Total Spent cases have both Price and Quantity present\n",
    "- Would require imputing Quantity first, introducing estimation error into a critical field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of rows with missing Total Spent (to be deleted):\n",
      "================================================================================\n",
      "    Transaction ID                            Category Item  Price Per Unit  \\\n",
      "6      TXN_1005543                                Food  NaN            30.5   \n",
      "64     TXN_1041483       Electric household essentials  NaN            15.5   \n",
      "65     TXN_1041890                           Furniture  NaN            27.5   \n",
      "104    TXN_1069238                                Food  NaN             5.0   \n",
      "180    TXN_1130015                       Milk Products  NaN             9.5   \n",
      "216    TXN_1153995       Electric household essentials  NaN            23.0   \n",
      "217    TXN_1154680                           Furniture  NaN            35.0   \n",
      "225    TXN_1158381                            Butchers  NaN            36.5   \n",
      "249    TXN_1175914                            Butchers  NaN            23.0   \n",
      "262    TXN_1187836  Computers and electric accessories  NaN            38.0   \n",
      "\n",
      "     Quantity  Total Spent  \n",
      "6         NaN          NaN  \n",
      "64        NaN          NaN  \n",
      "65        NaN          NaN  \n",
      "104       NaN          NaN  \n",
      "180       NaN          NaN  \n",
      "216       NaN          NaN  \n",
      "217       NaN          NaN  \n",
      "225       NaN          NaN  \n",
      "249       NaN          NaN  \n",
      "262       NaN          NaN  \n",
      "\n",
      "Observations about rows to be deleted:\n",
      "- All have missing Quantity (perfect overlap)\n",
      "- All have missing Item (100% overlap)\n",
      "- Cannot reconstruct Total Spent without Quantity\n",
      "- Represent systematic data collection gaps, not random missingness\n"
     ]
    }
   ],
   "source": [
    "# Display sample of rows to be dropped\n",
    "print('Sample of rows with missing Total Spent (to be deleted):')\n",
    "print('=' * 80)\n",
    "# Use mask to filter rows, then select columns\n",
    "print(df[df[TOTAL_SPENT].isna()][[TRANSACTION_ID, CATEGORY, ITEM, PRICE_PER_UNIT, QUANTITY, TOTAL_SPENT]].head(10))\n",
    "\n",
    "print('\\nObservations about rows to be deleted:')\n",
    "print('- All have missing Quantity (perfect overlap)')\n",
    "print('- All have missing Item (100% overlap)')\n",
    "print('- Cannot reconstruct Total Spent without Quantity')\n",
    "print('- Represent systematic data collection gaps, not random missingness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c93bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before deletion: 12575\n",
      "Rows to be deleted: 604\n",
      "\n",
      "Rows after deletion: 11971\n",
      "Rows deleted: 604\n",
      "Data retention rate: 95.20%\n"
     ]
    }
   ],
   "source": [
    "# Count rows before deletion\n",
    "# len(data) returns the total number of rows\n",
    "rowsBefore = len(df)\n",
    "print(f'Rows before deletion: {rowsBefore}')\n",
    "print(f'Rows to be deleted: {missingValue}')\n",
    "\n",
    "# Perform listwise deletion\n",
    "# .dropna removes rows with NaN values\n",
    "# subset=[TOTAL_SPENT] specifies to only check the Total Spent column\n",
    "# This removes any row where Total Spent is missing\n",
    "dataCleaned = df.dropna(subset=[TOTAL_SPENT])\n",
    "\n",
    "# Count rows after deletion\n",
    "rowsAfter = len(dataCleaned)\n",
    "# Calculate retention rate as percentage\n",
    "retentionRate = (rowsAfter / rowsBefore) * 100\n",
    "\n",
    "print(f'\\nRows after deletion: {rowsAfter}')\n",
    "print(f'Rows deleted: {rowsBefore - rowsAfter}')\n",
    "print(f'Data retention rate: {retentionRate:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f0829",
   "metadata": {},
   "source": [
    "### Validation: Side benefits of deletion\n",
    "\n",
    "Verify that deleting rows with missing `Total Spent` also eliminates other missing value problems, particularly with `Quantity`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value counts after Total Spent deletion:\n",
      "================================================================================\n",
      "Total Spent         :     0 ( 0.00%)\n",
      "Quantity            :     0 ( 0.00%)\n",
      "Price Per Unit      :   609 ( 5.09%)\n",
      "Item                :   609 ( 5.09%)\n",
      "\n",
      "✓ Total Spent is now 100% complete: True\n",
      "✓ Quantity is now 100% complete: True\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in numeric columns after deletion\n",
    "print('Missing value counts after Total Spent deletion:')\n",
    "print('=' * 80)\n",
    "\n",
    "# Iterate through critical numeric columns\n",
    "for col in [TOTAL_SPENT, QUANTITY, PRICE_PER_UNIT, ITEM]:\n",
    "    # dataCleaned[col].isna().sum() counts missing values in each column\n",
    "    missing_count = dataCleaned[col].isna().sum()\n",
    "    # Calculate percentage of missing values\n",
    "    missing_pct = (missing_count / len(dataCleaned)) * 100\n",
    "    # f-string formatting: {col:20s} pads column name to 20 characters for alignment\n",
    "    print(f'{col:20s}: {missing_count:5d} ({missing_pct:5.2f}%)')\n",
    "\n",
    "# Verify Total Spent is now complete\n",
    "print(f'\\n✓ Total Spent is now 100% complete: {dataCleaned[TOTAL_SPENT].isna().sum() == 0}')\n",
    "# Verify Quantity is now complete (due to perfect co-missingness)\n",
    "print(f'✓ Quantity is now 100% complete: {dataCleaned[QUANTITY].isna().sum() == 0}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92406b5d",
   "metadata": {},
   "source": [
    "### Mathematical consistency check\n",
    "\n",
    "Verify that for all complete rows, the relationship `Total Spent = Price Per Unit × Quantity` holds true. This validates data quality and ensures no mathematical inconsistencies exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with complete Price, Quantity, and Total Spent: 11362\n",
      "Rows with mathematical inconsistency (diff > 0.01): 0\n",
      "Mathematical consistency rate: 100.00%\n",
      "\n",
      "✓ All rows satisfy the formula: Total Spent = Price Per Unit × Quantity\n"
     ]
    }
   ],
   "source": [
    "# Create filter for rows where all three numeric fields are present\n",
    "# .notna() returns True where values are NOT missing\n",
    "# .all(axis=1) checks if all three conditions are True for each row\n",
    "completeRows = dataCleaned[[PRICE_PER_UNIT, QUANTITY, TOTAL_SPENT]].notna().all(axis=1)\n",
    "# Filter to only complete rows\n",
    "completeData = dataCleaned[completeRows].copy()\n",
    "\n",
    "print(f'Rows with complete Price, Quantity, and Total Spent: {len(completeData)}')\n",
    "\n",
    "# Calculate expected Total Spent using the formula\n",
    "# completeData['Price Per Unit'] * completeData['Quantity'] performs element-wise multiplication\n",
    "completeData['Calculated Total'] = completeData['Price Per Unit'] * completeData['Quantity']\n",
    "\n",
    "# Calculate absolute difference between actual and calculated\n",
    "# abs() returns absolute value (always positive)\n",
    "completeData['Difference'] = abs(completeData['Total Spent'] - completeData['Calculated Total'])\n",
    "\n",
    "# Count rows with significant differences (> 0.01 to account for floating point precision)\n",
    "# completeData['Difference'] > 0.01 creates boolean Series\n",
    "# .sum() counts True values\n",
    "inconsistent = (completeData['Difference'] > 0.01).sum()\n",
    "\n",
    "print(f'Rows with mathematical inconsistency (diff > 0.01): {inconsistent}')\n",
    "print(f'Mathematical consistency rate: {((len(completeData) - inconsistent) / len(completeData) * 100):.2f}%')\n",
    "\n",
    "if inconsistent == 0:\n",
    "    print('\\n✓ All rows satisfy the formula: Total Spent = Price Per Unit × Quantity')\n",
    "else:\n",
    "    print(f'\\n⚠ Warning: {inconsistent} rows have inconsistent calculations')\n",
    "    # Show sample of inconsistent rows for investigation\n",
    "    print('\\nSample of inconsistent rows:')\n",
    "    print(completeData[completeData['Difference'] > 0.01][[PRICE_PER_UNIT, QUANTITY, TOTAL_SPENT, 'Calculated Total', 'Difference']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74070ffc",
   "metadata": {},
   "source": [
    "### Impact on remaining missing values\n",
    "\n",
    "Analyze how deletion of Total Spent missing rows affects the overall missing data landscape, particularly for `Item` and `Price Per Unit` which will be handled in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decfbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact on Item missingness:\n",
      "================================================================================\n",
      "Item missing before: 1213\n",
      "Item missing after: 609\n",
      "Item missing rows removed: 604 (49.8% of Item missing cases)\n",
      "Item missing rows remaining: 609 (50.2%)\n",
      "\n",
      "Impact on Price Per Unit missingness:\n",
      "================================================================================\n",
      "Price Per Unit missing before: 609\n",
      "Price Per Unit missing after: 609\n",
      "Price Per Unit missing rows removed: 0 (0.0%)\n",
      "Price Per Unit missing rows remaining: 609 (100.0%)\n",
      "\n",
      "Key insight: Remaining missing values can now be imputed using available data:\n",
      "  - 609 Price Per Unit values can be reconstructed using Total ÷ Quantity\n",
      "  - 609 Item values can be imputed using Category information\n"
     ]
    }
   ],
   "source": [
    "print('Impact on Item missingness:')\n",
    "print('=' * 80)\n",
    "# Count missing Item values before and after deletion\n",
    "itemMissingBefore = df[ITEM].isna().sum()\n",
    "itemMissingAfter = dataCleaned[ITEM].isna().sum()\n",
    "itemRemoved = itemMissingBefore - itemMissingAfter\n",
    "itemRemovedPct = (itemRemoved / itemMissingBefore * 100) if itemMissingBefore else 0\n",
    "\n",
    "print(f'Item missing before: {itemMissingBefore}')\n",
    "print(f'Item missing after: {itemMissingAfter}')\n",
    "print(f'Item missing rows removed: {itemRemoved} ({itemRemovedPct:.1f}% of Item missing cases)')\n",
    "print(f'Item missing rows remaining: {itemMissingAfter} ({(itemMissingAfter / itemMissingBefore * 100) if itemMissingBefore else 0:.1f}%)')\n",
    "\n",
    "print('\\nImpact on Price Per Unit missingness:')\n",
    "print('=' * 80)\n",
    "# Count missing Price Per Unit values before and after deletion\n",
    "priceMissingBefore = df[PRICE_PER_UNIT].isna().sum()\n",
    "priceMissingAfter = dataCleaned[PRICE_PER_UNIT].isna().sum()\n",
    "priceRemoved = priceMissingBefore - priceMissingAfter\n",
    "priceRemovedPct = (priceRemoved / priceMissingBefore * 100) if priceMissingBefore else 0\n",
    "\n",
    "print(f'Price Per Unit missing before: {priceMissingBefore}')\n",
    "print(f'Price Per Unit missing after: {priceMissingAfter}')\n",
    "print(f'Price Per Unit missing rows removed: {priceRemoved} ({priceRemovedPct:.1f}%)')\n",
    "print(f'Price Per Unit missing rows remaining: {priceMissingAfter} ({(priceMissingAfter / priceMissingBefore * 100) if priceMissingBefore else 0:.1f}%)')\n",
    "\n",
    "print('\\nKey insight: Remaining missing values can now be imputed using available data:')\n",
    "print(f'  - {priceMissingAfter} Price Per Unit values can be reconstructed using Total ÷ Quantity')\n",
    "print(f'  - {itemMissingAfter} Item values can be imputed using Category information')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4d6fa",
   "metadata": {},
   "source": [
    "### Persist results\n",
    "\n",
    "Save the cleaned dataset with Total Spent missing rows removed. This becomes the input for subsequent imputation steps (Price Per Unit, then Item).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset with Total Spent missing rows removed saved to total_spent/../output_data/1_total_spent/total_spent_cleaned.csv\n",
      "  Final row count: 11971\n",
      "  Ready for next step: Price Per Unit imputation\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset to CSV\n",
    "# to_csv writes the DataFrame to a CSV file\n",
    "# index=False prevents writing row numbers as a column\n",
    "# This creates the output file that will be used in the next step (Price Per Unit imputation)\n",
    "dataCleaned.to_csv(CSV_OUT, index=False)\n",
    "print(f'✓ Dataset with Total Spent missing rows removed saved to total_spent/{CSV_OUT}')\n",
    "print(f'  Final row count: {len(dataCleaned)}')\n",
    "print(f'  Ready for next step: Price Per Unit imputation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02533bdf",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Total Spent Handling - STEP 1 Complete**\n",
    "\n",
    "**Classification:** MAR (Missing At Random)\n",
    "- Missingness depends on Item field (observable variable)\n",
    "- Perfect co-missingness with Quantity (100% overlap)\n",
    "\n",
    "**Method:** Listwise deletion\n",
    "- Removed 604 rows (4.8% of dataset)\n",
    "- Retained 95.2% of data\n",
    "\n",
    "**Justification:**\n",
    "- Total Spent is a critical target variable that should not be estimated\n",
    "- Cannot reconstruct: all missing cases also lack Quantity\n",
    "- Small data loss with large gain in data integrity\n",
    "\n",
    "**Side benefits:**\n",
    "- ✓ Quantity is now 100% complete (perfect co-missingness eliminated)\n",
    "- ✓ Reduced Item missing from 1,213 to 609 cases\n",
    "- ✓ All remaining missing values are now reconstructable/imputable\n",
    "\n",
    "**Next steps:**\n",
    "1. STEP 2: Impute Price Per Unit using formula (Total ÷ Quantity)\n",
    "2. STEP 3: Impute Item using mode by Category\n",
    "3. STEP 4: Handle Discount Applied as \"Unknown\" category\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
